import os
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# URL base para la descarga de archivos
BASE_URL = ""

# Directorio de destino para los archivos descargados
ROOT_DIR = "Archivos"

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.read()

async def download_file(session, url, dest_path):
    try:
        content = await fetch(session, url)
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        with open(dest_path, 'wb') as file:
            file.write(content)
        print(f"Descargado: {url}")
    except Exception as e:
        print(f"Error al descargar {url}: {e}")

async def download_directory(session, base_url, dest_dir):
    response = await fetch(session, base_url)
    soup = BeautifulSoup(response, 'html.parser')
    tasks = []
    
    for link in soup.find_all('a'):
        href = link.get('href')
        if href and href != '../':
            full_url = urljoin(base_url, href)
            path = urlparse(full_url).path.lstrip('/')
            
            if href.endswith('/'):
                # Crear subdirectorio correspondiente y continuar recursivamente
                new_dest_dir = os.path.join(dest_dir, href.strip('/'))
                await download_directory(session, full_url, new_dest_dir)
            else:
                # Descargar archivo en el directorio correspondiente
                dest_path = os.path.join(dest_dir, os.path.basename(path))
                tasks.append(download_file(session, full_url, dest_path))
    
    await asyncio.gather(*tasks)

async def main():
    global BASE_URL
    BASE_URL = input("Ingrese la URL a escanear: ").strip()
    os.makedirs(ROOT_DIR, exist_ok=True)
    
    async with aiohttp.ClientSession() as session:
        await download_directory(session, BASE_URL, ROOT_DIR)

if __name__ == "__main__":
    asyncio.run(main())
                           
